import{j as e}from"./index-D95GD6Bb.js";import"./styles--8pTCOHt.js";const i=()=>{const t={pageContainer:{backgroundColor:"#ffffff"},projectContainer:{backgroundColor:"#f0f4f8"},section:{backgroundColor:"white"}};return e.jsx("div",{className:"pageContainer",style:t.pageContainer,children:e.jsxs("div",{className:"projectContainer",style:t.projectContainer,children:[e.jsx("section",{className:"section",style:t.section,children:e.jsx("h1",{className:"header",children:"Virtual Arm with Multimodal Biased feedback for Improving EEG Motor Imagery Calibration Training "})}),e.jsxs("section",{className:"section",style:t.section,children:[e.jsx("h2",{className:"subHeader",children:"Keywords"}),e.jsxs("p",{className:"flex-1 text-justify mt-2",children:["1. Electroencephalogram (EEG), Brain Computer Interface (BCI), Motor Imagery (MI)",e.jsx("br",{}),"2. Machine Learning (ML), Support Vector Machine (SVM), PsychoPy"]})]}),e.jsxs("section",{className:"section",style:t.section,children:[e.jsx("h2",{className:"subHeader",children:"The Brain Computer Interface"}),e.jsxs("p",{className:"flex-1 text-justify mt-2",children:["Every action made by a person produces signals in the brain. These signals can be picked up by an electroencephalogram (EEG). Different actions result in different EEG signal patterns in the brain. By recording and deciphering the signals, we can tell what action the person intended to perform. In motor rehabilitation, we use these signals to allow users to communicate with their prosthetic arm. This is done through MI Brain Computer Interface (MI BCI) training, where the user learns to communicate with the BCI.",e.jsx("img",{src:"/jordan/images/bci/imagery.jpg",className:"image w-[50vh] h-auto mx-auto mt-5 center"}),e.jsx("br",{}),"However, MI BCIs demand long training spanning several weeks. This is because the training process is a counterintuitive task for patients. Most patients cannot visualize vivid imagery and a kinesthetic experience of the required actions. In our project, we demonstrate a setup involving the use of a virtual arm that acts out each action for a more intuitive experience.",e.jsx("img",{src:"/jordan/images/bci/arm.png",className:"image w-[50vh] h-auto mx-auto mt-5 center"})]})]}),e.jsxs("section",{className:"section",style:t.section,children:[e.jsx("h2",{className:"subHeader",children:"Experiment Design"}),e.jsxs("p",{className:"flex-1 text-justify mt-2",children:["In our setup, we used a wet EEG, which utilizes an electrically conductive gel to capture the participant's brain signals. A Tobii eye tracker was also employed to monitor the participant's eye movement. During the actual experiment, the lights are kept off to reduce external stimuli; they are only on here for demonstration purposes.",e.jsx("img",{src:"/jordan/images/bci/eeg_cap.jpg",className:"image w-[50vh] h-auto mx-auto mt-5 center"}),e.jsx("br",{}),"We recruited 13 participants, all of whom provided written informed consent and were monetarily compensated for their participation. Each participant underwent MI BCI training in two separate sessions—one with, and one without, the virtual arm—presented in random order. The training process is complex, and the detailed methodology can be found in our paper, but can be summarized in the diagram below.",e.jsx("img",{src:"/jordan/images/bci/block_diagram.png",className:"image w-[100vh] h-auto mx-auto mt-5 center"})]})]}),e.jsxs("section",{className:"section",style:t.section,children:[e.jsx("h2",{className:"subHeader",children:"Data Collection"}),e.jsxs("p",{className:"flex-1 text-justify mt-2",children:["The experiment is fully automated and paced by a Python program we made using PsychoPy. It displays on-screen instructions and visual stimuli required for MI tasks and communicates with the EEG cap, detailing indivisualized time stamps correlating with the user's pace for us to match up with the EEG data later.",e.jsx("img",{src:"/jordan/images/bci/stim.png",className:"image w-[100vh] h-auto mx-auto mt-5 center"}),e.jsx("br",{}),"We process the EEG signals by removing noise and unwanted data, followed by selecting and extracting important features to be used in classification. These features will be used to predict the user's intended action.",e.jsx("img",{src:"/jordan/images/bci/feature_extractor.png",className:"image w-[100vh] h-auto mx-auto mt-5 center"}),e.jsx("br",{}),"75% of the data collected will be used to train our Support Vector Machine (SVM) based Machine Leaning (ML) model, while the remaining 25% will be used to test the accuracy of the model.",e.jsx("img",{src:"/jordan/images/bci/feature_classification.png",className:"image w-[100vh] h-auto mx-auto mt-5 center"})]})]}),e.jsxs("section",{className:"section",style:t.section,children:[e.jsx("h2",{className:"subHeader",children:"Results"}),e.jsxs("p",{className:"flex-1 text-justify mt-2",children:["Classification accuracies across MI tasks ranged from 97.69% to 99.80%, demonstrating that our setup can indeed lead to high accuracy classification models.",e.jsx("img",{src:"/jordan/images/bci/results1.png",className:"image w-[100vh] h-auto mx-auto mt-5 center"})]})]}),e.jsxs("section",{className:"section",style:t.section,children:[e.jsx("h2",{className:"subHeader",children:"Links"}),e.jsxs("p",{className:"flex-1 text-justify mt-2",children:["Our poster:",e.jsx("a",{href:"/jordan/pdfs/bci_poster.pdf",target:"_blank",rel:"noopener noreferrer",style:{color:"#004d99",textDecoration:"underline",marginLeft:"10px"},children:"View Poster"}),e.jsx("br",{}),"Our research paper:",e.jsx("a",{href:"/jordan/pdfs/bci_report.pdf",target:"_blank",rel:"noopener noreferrer",style:{color:"#004d99",textDecoration:"underline",marginLeft:"10px"},children:"View Report"})]})]})]})})};export{i as default};
